{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pdf2image import convert_from_path\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "import json\n",
    "import ssl\n",
    "import certifi\n",
    "#from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import os\n",
    "import base64\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def pdf_to_images(pdf_path, dpi=200, output_folder=\"temp_images\"):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    # Convert PDF pages to a list of PIL Image objects\n",
    "    images = convert_from_path(pdf_path, dpi=dpi)\n",
    "    image_files = []\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_folder, f\"page_{i+1}.png\")\n",
    "        image.save(image_path, \"PNG\")\n",
    "        image_files.append(image_path)\n",
    "    return image_files\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Convert PDF to images\n",
    "    images_array = pdf_to_images(pdf_path)\n",
    "    responses = []\n",
    "    anthropic = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "    # Process each image\n",
    "    for image_path in images_array:\n",
    "        # Encode the image\n",
    "        base64_image = encode_image(image_path)\n",
    "        \n",
    "        try:\n",
    "            response = anthropic.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=1000,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": \"\"\"You are a tax expert. You will be provided with a document image, and your task is to extract all the text from it. \n",
    "                                Please don't add any additional information. Also only extract information from documents which are in the form of tax documents/bank statements etc instead of just plain text.\n",
    "                                Also I want you to process the output in the form of a json schema with as many fields as possible with values. \n",
    "                                There is no defined schema you need to extract as much info as you can in a json schema. Only return the information\n",
    "                                that exists as a NUMERICAL VALUE. If the value is not a number, then don't include it.\n",
    "                                BE AS PRECISE AS POSSIBLE.\n",
    "                                Take your time, the decision is yours, extract all the info CORRECTLY from this and give me back a json and not a string\"\"\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image\",\n",
    "                                \"source\": {\n",
    "                                    \"type\": \"base64\",\n",
    "                                    \"media_type\": \"image/png\",\n",
    "                                    \"data\": base64_image\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Extract content from the response\n",
    "            responses.append(response.content[0].text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {str(e)}\")\n",
    "            \n",
    "    # Clean up temporary image files\n",
    "    for image_path in images_array:\n",
    "        try:\n",
    "            os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing temporary file {image_path}: {str(e)}\")\n",
    "            \n",
    "    # Join text from all pages\n",
    "    extracted_text = \"\\n\\n\".join(responses)\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "loda = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/mohak/Desktop/Hacklytics/taxerino/backend/extraction/payslip-1740283884.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "data2 = json.loads(extracted_text)\n",
    "loda['file1'] = data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/mohak/Desktop/Hacklytics/taxerino/backend/extraction/edited_W2 Form.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "data = json.loads(extracted_text)\n",
    "loda['file2'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': {'employer_identification_number': 19202020,\n",
       "  'wages_and_compensation': 94900,\n",
       "  'federal_income_tax_withheld': 27450,\n",
       "  'tax_year': 2025},\n",
       " 'file2': {'employer_identification_number': 19202021,\n",
       "  'wages_and_compensation': 94900,\n",
       "  'federal_income_tax_withheld': 37450,\n",
       "  'tax_year': 2025}}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'employer_identification_number': {'ein_from_file_1': 19202020,\n",
       "  'ein_from_file_2': 192020021,\n",
       "  'ein_anomaly': 'mismatch detected'},\n",
       " 'federal_income_tax_withheld': {'tax_withheld_from_file_1': 27450,\n",
       "  'tax_withheld_from_file_2': 37450,\n",
       "  'tax_withheld_anomaly': 10000}}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def anomaly_detection(json_data):\n",
    "    '''\n",
    "    Function to detect anomalies in the data\n",
    "\n",
    "    checks for common fields in the data provided upon inserting forms on webiste and tallies them to check whether the numbers match\n",
    "    uses gpt wrapper prevent having to check manually each field\n",
    "    takes in json data and returns a json object with the anomalies detected\n",
    "    '''\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \n",
    "                \"\"\"\n",
    "                    You are the smartest, most witty, sharp eyed and sharp minded \n",
    "                    accountant, with the brain speed of a super computer and expertise far beyond \n",
    "                    any human or other computer. Your job is to analyse the given json data containing\n",
    "                    financial information from the user, which may either just be a singular file, or \n",
    "                    multiple. Your job is to check data across similar fields and check if they match/tally up.\n",
    "\n",
    "                    AS MERELY AN EXAMPLE, if the user has entered 2 financial docs with their salary, and if the \n",
    "                    salaries are not the same, then you should raise this anomaly. ANOTHER MERE EXAMPLE MAY BE \n",
    "                    if the user has summed up how much tax they filed in a year but the individual tax fields do \n",
    "                    not match with the amount filed, then you should raise this anomaly.\n",
    "\n",
    "                    When it comes to returning the data, it should be in a json format, with the key being the umbrella\n",
    "                    field that sees the anomaly, and the value should contain a dictionary of as much information \n",
    "                    concerning the anomaly as possible.\n",
    "\n",
    "                    A final example with the proper format is as follows. keep in mind all these values are just examples, \n",
    "                    the output must be in a similar format but json file.:\n",
    "                    {\n",
    "                        'salary':\n",
    "                            {\n",
    "                            \"salary_from_file_1\": {\n",
    "                                \"salary_1\": 100_000,\n",
    "                                \"tax_1\": 10_000,\n",
    "                                \"salary_after_tax_1\": 90_000\n",
    "                            },\n",
    "                            \"salary_from_file_2\": {\n",
    "                                \"salary_2\": 120_000,\n",
    "                                \"tax_2\": 10_000,\n",
    "                                \"salary_after_tax_2\": 110_000\n",
    "                            },\n",
    "                            \"salary_anomaly\":{\n",
    "                                \"salary_1\": 100_000,\n",
    "                                \"salary_2\": 120_000,\n",
    "                                \"salary_anomaly\": 20_000\n",
    "                            },\n",
    "                            \"salary_after_tax_anomaly\":{\n",
    "                                \"salary_after_tax_1\": 90_000,\n",
    "                                \"salary_after_tax_2\": 110_000,\n",
    "                                \"salary_after_tax_anomaly\": 20_000\n",
    "                            }\n",
    "                        },\n",
    "                        'other_field':\n",
    "                            {\n",
    "                                \"field_1\": \"value_1\",\n",
    "                                \"field_2\": \"value_2\",\n",
    "                                \"field_anomaly\": \"value_anomaly\"\n",
    "                            }\n",
    "                    }\n",
    "                #NOTE: ONLY RETURN THE FIELDS THAT ACTUALLY HAVE ANOMALIES, NOT ALL FIELDS\n",
    "                #ANOTHER NOTE: for quantifable numerical data find the differences but for values like EIN that are not quantifable, just return the string \"mismatch detected\"\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": str(json_data)\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    output = completion.choices[0].message.content\n",
    "    return json.loads(output)\n",
    "anomaly_detection(loda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided data, I will create two lists: one for the transaction dates and one for the corresponding transaction amounts. These can be used for further data analysis or visualization.\n",
      "\n",
      "1. **Transaction Dates:**\n",
      "   - Deposit: '05-15'\n",
      "   - ATM Withdrawal: '05-18'\n",
      "   - Check Paid: '05-12'\n",
      "   - Check Paid: '05-18'\n",
      "   - Check Paid: '05-24'\n",
      "\n",
      "2. **Transaction Amounts:**\n",
      "   - Deposit: 3615.08\n",
      "   - ATM Withdrawal: 20.00\n",
      "   - Check Paid: 75.00\n",
      "   - Check Paid: 30.00\n",
      "   - Check Paid: 200.00\n",
      "\n",
      "These lists show transactions across different days in May, with the corresponding financial amounts. You can use these lists to create plots or graphs to visualize cash flow or expenditure over the timeframe of the bank statement.\n"
     ]
    }
   ],
   "source": [
    "def extract_insights(json_data):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a helpful finance/tax assistant who recieves json formatted data.\n",
    "         The data will be related to finance/tax and your job is to provide me with the data that I can use to perfrom some nice visualizations\n",
    "         give me the result in the form of 2 lists on which I can peform some data analysis/viz\"\"\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": str(combined_json)\n",
    "        }\n",
    "    ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "    \n",
    "print(extract_insights(combined_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_folder = '/Users/abhyudaygoyal/Desktop/HACKLYTICS/taxerino/backend/uploads'\n",
    "all_data = {}\n",
    "pdf_files = [f for f in os.listdir(upload_folder) if f.endswith('.pdf')]\n",
    "print(pdf_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
